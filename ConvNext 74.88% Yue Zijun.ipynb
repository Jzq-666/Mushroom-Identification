{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPzwWD6fRn27"
      },
      "source": [
        "# ConvNext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WJmEPeMLhhD"
      },
      "outputs": [],
      "source": [
        "# Install the torchinfo package for showing the network architecture information\n",
        "!pip install torchinfo -qqq\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzeLfrQXLsK-"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries for working with CIFART-10 dataset and PyTorch.\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn,optim,no_grad\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchinfo import summary\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ja96rnbLvVa"
      },
      "outputs": [],
      "source": [
        "# To configure the usage of a GPU (cuda) or MPS (Apple) if either of them is available\n",
        "has_mps = torch.backends.mps.is_built()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if has_mps else \"cpu\"\n",
        "print(f\"Python versoin: {sys.version_info.major, sys.version_info.minor, sys.version_info.micro}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ3n91k63J_o"
      },
      "outputs": [],
      "source": [
        "# 定义 drop_path 函数\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    output = x / keep_prob * random_tensor\n",
        "    return output\n",
        "\n",
        "# 定义 ConvBNAct 类用于卷积、BN 和激活函数的组合\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, kernel_size, stride, act_layer=nn.SiLU):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size // 2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_chs)\n",
        "        self.act = act_layer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "# 定义 SqueezeExcite 模块\n",
        "class SqueezeExcite(nn.Module):\n",
        "    def __init__(self, in_chs, se_ratio=0.25):\n",
        "        super(SqueezeExcite, self).__init__()\n",
        "        reduced_chs = max(1, int(in_chs * se_ratio))\n",
        "        self.fc1 = nn.Conv2d(in_chs, reduced_chs, 1)\n",
        "        self.fc2 = nn.Conv2d(reduced_chs, in_chs, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scale = F.adaptive_avg_pool2d(x, 1)\n",
        "        scale = torch.sigmoid(self.fc2(F.silu(self.fc1(scale))))\n",
        "        return x * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY_mG4n6gZ9d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGX0JOMPgtTD"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/trainset'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/testset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaZn9ER2L4Gl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# 数据增强部分\n",
        "train_transform = transforms.Compose([\n",
        "    # 数据增强部分增加了旋转和随机仿射变换\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(degrees=45),  # 增加旋转\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    #transforms.RandomAffine(degrees=10, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=20),  # 增加仿射变换\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 测试集的数据增强保持不变\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 加载数据集\n",
        "train_set = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
        "test_set = datasets.ImageFolder(root=test_path, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the classes if they are known\n",
        "classes = [\n",
        "    '云芝', '冬菇', '冬虫夏草', '变绿红菇', '大青褶伞', '大鹿花菌', '宽鳞多孔菌',\n",
        "    '尖顶地星', '干巴菌', '杏鮑菇', '毒丝盖伞', '胶质刺银耳', '毒蝇伞', '毛头鬼伞', '灵芝',\n",
        "    '牛舌菌', '狭头小菇', '猴头菇', '硫黄菌', '竹荪', '粉红枝瑚菌', '粪生黑蛋巢菌',\n",
        "    '紫蜡蘑', '红紫柄小菇', '红菇', '蓝绿乳菇', '羊肚菌', '美味牛肝菌', '裂褶菌',\n",
        "     '赭红拟口蘑',  '金黃鵝膏菌', '欧洲黑木耳',\n",
        "    '鹿蕊', '鳞柄白鹅膏', '黄裙竹荪', '黑松露'\n",
        "]\n",
        "\n",
        "# Check dataset loading\n",
        "print(\"Number of training samples:\", len(train_set))\n",
        "print(\"Number of test samples:\", len(test_set))\n",
        "print(\"Classes:\", classes)\n",
        "print(\"Class to index mapping:\", train_set.class_to_idx)\n",
        "print(len(classes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8HYVBFAaNlx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 下载并安装中文字体\n",
        "!apt-get -qq install -y fonts-noto-cjk\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 查找可用的 Noto 字体\n",
        "font_dirs = fm.findSystemFonts(fontpaths=None)\n",
        "print(\"Available fonts:\", font_dirs)\n",
        "\n",
        "# 设置使用的中文字体\n",
        "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"  # 正确的字体路径\n",
        "font_prop = fm.FontProperties(fname=font_path)\n",
        "\n",
        "# 获取类别数\n",
        "num_classes = len(classes)\n",
        "classes = train_set.classes\n",
        "print(len(train_set.classes))\n",
        "\n",
        "# 确保 classes 和 train_set.class_to_idx 对应\n",
        "class_idx_to_chinese = {v: classes[v] for v in range(len(classes))}\n",
        "\n",
        "# 每个类别选择一张示例图像\n",
        "sample_images = []\n",
        "for label in range(len(classes)):\n",
        "    # 找到第一个属于当前类别的图像\n",
        "    for path, target in train_set.imgs:\n",
        "        if target == label:\n",
        "            img = Image.open(path).convert(\"RGB\")  # 转换为RGB\n",
        "            sample_images.append((img, class_idx_to_chinese[label]))  # 用中文名作为标签\n",
        "            break\n",
        "\n",
        "# 设置显示网格\n",
        "fig, axes = plt.subplots(4, 10, figsize=(20, 8))\n",
        "i = 0\n",
        "for row in axes:\n",
        "    for axis in row:\n",
        "        axis.set_xticks([])\n",
        "        axis.set_yticks([])\n",
        "        if i < len(sample_images):\n",
        "            img, label = sample_images[i]\n",
        "            axis.set_xlabel(label, fontsize=12, fontproperties=font_prop)  # 中文标签\n",
        "            axis.imshow(img)\n",
        "        else:\n",
        "            axis.axis('off')  # 如果样本不足，关闭多余的子图框\n",
        "        i += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6xrNFO3JqN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVTiYFo-64hL"
      },
      "source": [
        "\n",
        "**Define CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEK3e_Mw7BKF"
      },
      "outputs": [],
      "source": [
        "# Define the model of ConvNeXt\n",
        "class ConvNeXtModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNeXtModel, self).__init__()\n",
        "        # Load a pre-trained ConvNeXt model\n",
        "        self.model = models.convnext_base(weights='DEFAULT')  # Use available weights\n",
        "        # Replace the classifier head for CIFAR-10\n",
        "        self.model.classifier[2] = nn.Linear(self.model.classifier[2].in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd4ZMQVJ_sLE"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IYUttEN7KaQ"
      },
      "outputs": [],
      "source": [
        "model = ConvNeXtModel(num_classes=10).to(device)\n",
        "\n",
        "summary(model=model, input_size=(1, 3, 32, 32), col_width=15,\n",
        "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "        row_settings=['var_names'], verbose=0)\n",
        "# Create a Models folder to store the checkpoints\n",
        "!mkdir Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfE9_hyN_wb2"
      },
      "outputs": [],
      "source": [
        "# Specify Loss/Cost function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.004)\n",
        "\n",
        "# Specify Learning Rate Scheduler\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3, verbose=True, min_lr=1e-6)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWQ85N5k_wJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Start Training\n",
        "EPOCHS = 300\n",
        "#initialize early stopping variables\n",
        "best_acc = 0.0\n",
        "patience =5\n",
        "patience_counter =0\n",
        "\n",
        "\n",
        "loss_hist, acc_hist = [], []  # Lists to store training loss and accuracy\n",
        "loss_hist_test, acc_hist_test = [], []  # Lists to store validation loss and accuracy\n",
        "\n",
        "model.to(device)  # Move the model to the specified device (e.g., GPU)\n",
        "\n",
        "print(\"Training was started.\\n\")\n",
        "\n",
        "# Warm-up for 5 epochs\n",
        "for epoch in range(5):\n",
        "    # Train with a small learning rate\n",
        "    optimizer.param_groups[0]['lr'] = 1e-4  # Start with a lower learning rate\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    time_ckpt = time.time()\n",
        "    print(\"EPOCH:\", epoch, end=\" \")\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    # Training loop\n",
        "    for data in train_loader:\n",
        "        batch, labels = data\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "        outputs = model(batch)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass (compute gradients)\n",
        "        optimizer.step()  # Update the model's parameters\n",
        "\n",
        "        # Compute training statistics\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_set)  # Average training loss for the epoch\n",
        "    avg_acc = correct / len(train_set)  # Average training accuracy for the epoch\n",
        "    loss_hist.append(avg_loss)\n",
        "    acc_hist.append(avg_acc)\n",
        "\n",
        "    # Validation statistics\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        loss_test = 0.0\n",
        "        correct_test = 0\n",
        "\n",
        "        # Validation loop\n",
        "        for data in test_loader:\n",
        "            batch, labels = data\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            loss_test += loss.item()\n",
        "\n",
        "        avg_loss_test = loss_test / len(test_set)  # Average validation loss for the epoch\n",
        "        avg_acc_test = correct_test / len(test_set)  # Average validation accuracy for the epoch\n",
        "        loss_hist_test.append(avg_loss_test)\n",
        "        acc_hist_test.append(avg_acc_test)\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "#     scheduler.step(avg_loss_val) # Check the scheduler for updating the learning rate\n",
        "\n",
        "    # Save the model at the end of each epoch\n",
        "    with open(\"Models/lenet5_model_{}.pth\".format(epoch), \"wb\") as f:\n",
        "        model.eval()\n",
        "        pickle.dump(model, f)\n",
        "        model.train()\n",
        "    # Early Stopping Logic\n",
        "    if avg_acc_test > best_acc:\n",
        "        best_acc = avg_acc_test\n",
        "        patience_counter = 0\n",
        "        # Save the model at the end of the epoch\n",
        "        with open(f\"Models/best_model_epoch_{epoch}.pth\", \"wb\") as f:\n",
        "            pickle.dump(model, f)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    print(\"Train Loss: {:.3f}\".format(avg_loss * 100), end=\" \")\n",
        "    print(\"Test Loss: {:.3f}\".format(avg_loss_test * 100), end=\" \")\n",
        "    print(\"Train Accuracy: {:.2f}%\".format(avg_acc * 100), end=\" \")\n",
        "    print(\"Test Accuracy: {:.2f}%\".format(avg_acc_test * 100), end=\" \")\n",
        "    print(\"Time: {:.2f}s\".format(time.time() - time_ckpt), end=\" \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxfxMIYB_wAc"
      },
      "outputs": [],
      "source": [
        "plots=[(loss_hist,loss_hist_test),(acc_hist,acc_hist_test)]\n",
        "plt_labels=[(\"Training Loss\",\"Test Loss\"),(\"Training Accuracy\",\"Test Accuracy\")]\n",
        "plt_titles=[\"Loss\",\"Accuracy\"]\n",
        "plt.figure(figsize=(20,7))\n",
        "for i in range(0,2):\n",
        "    ax=plt.subplot(1,2,i+1)\n",
        "    ax.plot(plots[i][0],label=plt_labels[i][0])\n",
        "    ax.plot(plots[i][1],label=plt_labels[i][1])\n",
        "    ax.set_title(plt_titles[i])\n",
        "    ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhIEDzx-AAAT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Selecting the best model\n",
        "best_acc = max(acc_hist_test)\n",
        "best_epoch = acc_hist_test.index(best_acc)+1\n",
        "\n",
        "print(\"Best accuracy on test set: {:.2f}%\".format(best_acc*100))\n",
        "print(\"Best epoch: {}\".format(best_epoch))\n",
        "\n",
        "# Load the best model\n",
        "with open(f\"Models/shuffleNetv2_model_{best_epoch}.pth\",\"rb\") as f:\n",
        "    model=pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbTTQayTAE1_"
      },
      "outputs": [],
      "source": [
        "pred_vec = []\n",
        "label_vec = []\n",
        "correct = 0\n",
        "test_loss = 0.0\n",
        "avg_test_loss = 0.0\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        batch, labels = data\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        outputs = model(batch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        test_loss += loss.item()\n",
        "        pred_vec.extend(predicted.cpu().numpy())  # Convert tensor to numpy array\n",
        "        label_vec.extend(labels.cpu().numpy())  # Convert tensor to numpy array\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_set)\n",
        "\n",
        "pred_vec = np.array(pred_vec)\n",
        "label_vec = np.array(label_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWVRmHvsAH5Y"
      },
      "outputs": [],
      "source": [
        "print(F\"Test Loss: {avg_test_loss}\")\n",
        "print(F\"Test Accuracy on the {len(test_set)} test images: {(100 * correct / len(test_set))}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE4dGH1tALT1"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_mat = confusion_matrix(label_vec, pred_vec)\n",
        "# Convert confusion matrix to pandas DataFrame\n",
        "labels = np.unique(label_vec)\n",
        "confusion_df = pd.DataFrame(confusion_mat, index=classes, columns=classes)\n",
        "print(\"Confusion Matrix\")\n",
        "confusion_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgXzv2mlAXbq"
      },
      "outputs": [],
      "source": [
        "# Create a report to show the f1-score, precision, recall\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = pd.DataFrame.from_dict(classification_report(pred_vec,label_vec,output_dict=True)).T\n",
        "report['Label']=[classes[int(x)] if x.isdigit() else \" \" for x in report.index]\n",
        "report=report[['Label','f1-score','precision','recall','support']]\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZGbdBBBMiwb3"
      },
      "outputs": [],
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRApu_RnAa6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 下载并安装中文字体\n",
        "!apt-get -qq install -y fonts-noto-cjk\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 查找可用的 Noto 字体\n",
        "font_dirs = fm.findSystemFonts(fontpaths=None)\n",
        "print(\"Available fonts:\", font_dirs)\n",
        "\n",
        "# 设置使用的中文字体\n",
        "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"  # 正确的字体路径\n",
        "font_prop = fm.FontProperties(fname=font_path)\n",
        "# obtain one batch of test images\n",
        "images, labels = next(iter(test_loader))\n",
        "model.cpu()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "\n",
        "# Create a 4x4 grid for displaying the images\n",
        "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "\n",
        "# Iterate over the images and display them in the grid\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "  # Normalize the image tensor to [0, 1] range\n",
        "  image = images[idx].permute(1, 2, 0)\n",
        "  image = (image - image.min()) / (image.max() - image.min())\n",
        "  ax.imshow(image)  # Display the image\n",
        "  ax.axis('off')  # Hide the axes\n",
        "  ax.set_title(\"{}\".format(classes[preds[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))  # Add title to the image\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2V6WHajAayd"
      },
      "outputs": [],
      "source": [
        "# Define the loader for all test data\n",
        "test_set = datasets.ImageFolder(root=test_path, transform=test_transform)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_set)\n",
        "images, labels = next(iter(test_loader))\n",
        "model.cpu()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "for idx in range(50):\n",
        "    # Normalize the image tensor to [0, 1] range\n",
        "    image = images[idx].permute(1, 2, 0)\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    ax = fig.add_subplot(5, 10, idx + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(image, interpolation='nearest')\n",
        "\n",
        "    if preds[idx]==labels[idx]:\n",
        "      ax.text(0, 3, str(classes[preds[idx].item()]), color='green')\n",
        "    else:\n",
        "      ax.text(0, 3, str(classes[preds[idx].item()]), color='red')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9-BDAC7AavU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualize wrongly classified image for each class\n",
        "pred_vec_all = []\n",
        "correct = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader_all:\n",
        "        batch, labels = data\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        outputs = model(batch)\n",
        "        test_loss=criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        pred_vec_all.append(predicted)\n",
        "    pred_vec_all = torch.cat(pred_vec_all)\n",
        "\n",
        "pred_vec_all = pred_vec_all.cpu().numpy()\n",
        "ground_truths = np.asarray(test_set_all.targets)\n",
        "incorrect_mask = pred_vec_all != ground_truths\n",
        "incorrect_images = [test_set_all.data[(ground_truths == label) & incorrect_mask][0] for label in range(10)]\n",
        "pred_results_all = [pred_vec_all[(ground_truths == label) & incorrect_mask][0] for label in range(10)]\n",
        "\n",
        "# show images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "i = 0\n",
        "for row in axes:\n",
        "  for axis in row:\n",
        "    axis.set_xticks([])\n",
        "    axis.set_yticks([])\n",
        "    axis.set_xlabel(\"Predicted: %s\" % classes[pred_results_all[i]], fontsize=10)\n",
        "    axis.imshow(incorrect_images[i], cmap='gray')\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ISyKBXt9bZV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}